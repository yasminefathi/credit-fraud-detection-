{
  "cells": [
    {
      "source": [
        "\n",
        "# IMPORTANT: RUN THIS CELL IN ORDER TO IMPORT YOUR KAGGLE DATA SOURCES\n",
        "# TO THE CORRECT LOCATION (/kaggle/input) IN YOUR NOTEBOOK,\n",
        "# THEN FEEL FREE TO DELETE THIS CELL.\n",
        "# NOTE: THIS NOTEBOOK ENVIRONMENT DIFFERS FROM KAGGLE'S PYTHON\n",
        "# ENVIRONMENT SO THERE MAY BE MISSING LIBRARIES USED BY YOUR\n",
        "# NOTEBOOK.\n",
        "\n",
        "import os\n",
        "import sys\n",
        "from tempfile import NamedTemporaryFile\n",
        "from urllib.request import urlopen\n",
        "from urllib.parse import unquote, urlparse\n",
        "from urllib.error import HTTPError\n",
        "from zipfile import ZipFile\n",
        "import tarfile\n",
        "import shutil\n",
        "\n",
        "CHUNK_SIZE = 40960\n",
        "DATA_SOURCE_MAPPING = ':https%3A%2F%2Fstorage.googleapis.com%2Fkaggle-data-sets%2F310%2F23498%2Fbundle%2Farchive.zip%3FX-Goog-Algorithm%3DGOOG4-RSA-SHA256%26X-Goog-Credential%3Dgcp-kaggle-com%2540kaggle-161607.iam.gserviceaccount.com%252F20240512%252Fauto%252Fstorage%252Fgoog4_request%26X-Goog-Date%3D20240512T143751Z%26X-Goog-Expires%3D259200%26X-Goog-SignedHeaders%3Dhost%26X-Goog-Signature%3D217c0014abec59e63cf4258eca3749053cf11bb643bb0b1d0a5953e405733db8c003c6902874b61b2980d4490ce58b0faccb7dd8f9e2bc2c263c236fccbf6c82d075426955c1dfb034b2af64a88821911718b44b8785999eea15768ee2f364d32aaaa195e198db07df082c33ebab83539f8d2451212b0df9b143aefbb62e959b2f25587336856654b6bfc7866ae167c27d937576a86e840076ff0af0d3abb5e58d8847de43d25fe3f2e2cfec0c266696036bf4529f5f66843fab86b987ecd669d90060fbde07b0a3220cfb31854790daad7dba67a654330d5249f1b3e792cf22cc833e83f27a775c9997d1657121d4a712b1f72c8138346e45dc3cef523e7445'\n",
        "\n",
        "KAGGLE_INPUT_PATH='/kaggle/input'\n",
        "KAGGLE_WORKING_PATH='/kaggle/working'\n",
        "KAGGLE_SYMLINK='kaggle'\n",
        "\n",
        "!umount /kaggle/input/ 2> /dev/null\n",
        "shutil.rmtree('/kaggle/input', ignore_errors=True)\n",
        "os.makedirs(KAGGLE_INPUT_PATH, 0o777, exist_ok=True)\n",
        "os.makedirs(KAGGLE_WORKING_PATH, 0o777, exist_ok=True)\n",
        "\n",
        "try:\n",
        "  os.symlink(KAGGLE_INPUT_PATH, os.path.join(\"..\", 'input'), target_is_directory=True)\n",
        "except FileExistsError:\n",
        "  pass\n",
        "try:\n",
        "  os.symlink(KAGGLE_WORKING_PATH, os.path.join(\"..\", 'working'), target_is_directory=True)\n",
        "except FileExistsError:\n",
        "  pass\n",
        "\n",
        "for data_source_mapping in DATA_SOURCE_MAPPING.split(','):\n",
        "    directory, download_url_encoded = data_source_mapping.split(':')\n",
        "    download_url = unquote(download_url_encoded)\n",
        "    filename = urlparse(download_url).path\n",
        "    destination_path = os.path.join(KAGGLE_INPUT_PATH, directory)\n",
        "    try:\n",
        "        with urlopen(download_url) as fileres, NamedTemporaryFile() as tfile:\n",
        "            total_length = fileres.headers['content-length']\n",
        "            print(f'Downloading {directory}, {total_length} bytes compressed')\n",
        "            dl = 0\n",
        "            data = fileres.read(CHUNK_SIZE)\n",
        "            while len(data) > 0:\n",
        "                dl += len(data)\n",
        "                tfile.write(data)\n",
        "                done = int(50 * dl / int(total_length))\n",
        "                sys.stdout.write(f\"\\r[{'=' * done}{' ' * (50-done)}] {dl} bytes downloaded\")\n",
        "                sys.stdout.flush()\n",
        "                data = fileres.read(CHUNK_SIZE)\n",
        "            if filename.endswith('.zip'):\n",
        "              with ZipFile(tfile) as zfile:\n",
        "                zfile.extractall(destination_path)\n",
        "            else:\n",
        "              with tarfile.open(tfile.name) as tarfile:\n",
        "                tarfile.extractall(destination_path)\n",
        "            print(f'\\nDownloaded and uncompressed: {directory}')\n",
        "    except HTTPError as e:\n",
        "        print(f'Failed to load (likely expired) {download_url} to path {destination_path}')\n",
        "        continue\n",
        "    except OSError as e:\n",
        "        print(f'Failed to load {download_url} to path {destination_path}')\n",
        "        continue\n",
        "\n",
        "print('Data source import complete.')\n"
      ],
      "metadata": {
        "id": "7U-O04sAPScw",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "64d7ad86-f515-4aa1-80b6-8d4a370fe167"
      },
      "cell_type": "code",
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Failed to load (likely expired) https://storage.googleapis.com/kaggle-data-sets/310/23498/bundle/archive.zip?X-Goog-Algorithm=GOOG4-RSA-SHA256&X-Goog-Credential=gcp-kaggle-com%40kaggle-161607.iam.gserviceaccount.com%2F20240512%2Fauto%2Fstorage%2Fgoog4_request&X-Goog-Date=20240512T143751Z&X-Goog-Expires=259200&X-Goog-SignedHeaders=host&X-Goog-Signature=217c0014abec59e63cf4258eca3749053cf11bb643bb0b1d0a5953e405733db8c003c6902874b61b2980d4490ce58b0faccb7dd8f9e2bc2c263c236fccbf6c82d075426955c1dfb034b2af64a88821911718b44b8785999eea15768ee2f364d32aaaa195e198db07df082c33ebab83539f8d2451212b0df9b143aefbb62e959b2f25587336856654b6bfc7866ae167c27d937576a86e840076ff0af0d3abb5e58d8847de43d25fe3f2e2cfec0c266696036bf4529f5f66843fab86b987ecd669d90060fbde07b0a3220cfb31854790daad7dba67a654330d5249f1b3e792cf22cc833e83f27a775c9997d1657121d4a712b1f72c8138346e45dc3cef523e7445 to path /kaggle/input/\n",
            "Data source import complete.\n"
          ]
        }
      ],
      "execution_count": null
    },
    {
      "cell_type": "markdown",
      "source": [
        "# ðŸ’³ Credit Card Fraud Detection Intuitions\n",
        "\n",
        "## What is Credit Card Fraud?\n",
        "Credit card fraud is when someone uses another person's credit card or account information to make unauthorized purchases or access funds through cash advances. Credit card fraud doesnâ€™t just happen online; it happens in brick-and-mortar stores, too. As a business owner, you can avoid serious headaches â€“ and unwanted publicity â€“ by recognizing potentially fraudulent use of credit cards in your payment environment.\n",
        "\n",
        "## Three challenges surrounding credit card fraud\n",
        "\n",
        "1. It's not always easy to agree on ground truth for what \"fraud\" means.\n",
        "2. Regardless of how you define ground truth, the vast majority of charges are not fraudulent.\n",
        "3. Most merchants aren't experts at evaluating the business impact of fraud.\n",
        "\n"
      ],
      "metadata": {
        "id": "RxgHUAGUZHXR"
      }
    },
    {
      "metadata": {
        "_cell_guid": "ae8dd7f3-80a7-4db9-a132-823b0e48c041",
        "_uuid": "c999e5f1ac81513263d83883008f2844209e9e07",
        "id": "ztrtuF_MPSc0"
      },
      "cell_type": "markdown",
      "source": [
        "## Gather Sense of Our Data:\n",
        "<a id=\"gather\"></a>\n",
        "The first thing we must do is gather a <b> basic sense </b> of our data. Remember, except for the <b>transaction</b> and <b>amount</b> we dont know what the other columns are (due to privacy reasons). The only thing we know, is that those columns that are unknown have been scaled already.   \n",
        "\n",
        "<h3> Summary: </h3>\n",
        "<ul>\n",
        "<li>The transaction amount is relatively <b>small</b>. The mean of all the mounts made is approximately USD 88. </li>\n",
        "<li>There are no <b>\"Null\"</b> values, so we don't have to work on ways to replace values. </li>\n",
        "<li> Most of the transactions were <b>Non-Fraud</b> (99.83%) of the time, while <b>Fraud</b> transactions occurs (017%) of the time in the dataframe. </li>\n",
        "</ul>\n",
        "\n",
        "<h3> Feature Technicalities: </h3>\n",
        "<ul>\n",
        "<li> <b>PCA Transformation: </b>  The description of the data says that all the features went through a PCA transformation (Dimensionality Reduction technique) (Except for time and amount).</li>\n",
        "<li> <b>Scaling:</b> Keep in mind that in order to implement a PCA transformation features need to be previously scaled. (In this case, all the V features have been scaled or at least that is what we are assuming the people that develop the dataset did.)</li>\n",
        "</ul>"
      ]
    },
    {
      "metadata": {
        "_cell_guid": "b1076dfc-b9ad-4769-8c92-a6c4dae69d19",
        "_uuid": "8f2839f25d086af736a60e9eeb907d3b93b6e0e5",
        "trusted": true,
        "id": "qNcWFKUjPSc1",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 356
        },
        "outputId": "92db987c-7ff9-452a-a1b2-d04ae087da84"
      },
      "cell_type": "code",
      "source": [
        "# This Python 3 environment comes with many helpful analytics libraries installed\n",
        "# It is defined by the kaggle/python docker image: https://github.com/kaggle/docker-python\n",
        "# For example, here's several helpful packages to load in\n",
        "\n",
        "# Imported Libraries\n",
        "\n",
        "import numpy as np # linear algebra\n",
        "import pandas as pd # data processing, CSV file I/O (e.g. pd.read_csv)\n",
        "import tensorflow as tf\n",
        "import matplotlib.pyplot as plt\n",
        "import seaborn as sns\n",
        "from sklearn.manifold import TSNE\n",
        "from sklearn.decomposition import PCA, TruncatedSVD\n",
        "import matplotlib.patches as mpatches\n",
        "import time\n",
        "\n",
        "# Classifier Libraries\n",
        "from sklearn.linear_model import LogisticRegression\n",
        "\n",
        "import collections\n",
        "\n",
        "\n",
        "# Other Libraries\n",
        "from sklearn.model_selection import train_test_split\n",
        "from sklearn.pipeline import make_pipeline\n",
        "from imblearn.pipeline import make_pipeline as imbalanced_make_pipeline\n",
        "from imblearn.over_sampling import SMOTE\n",
        "from imblearn.under_sampling import NearMiss\n",
        "from imblearn.metrics import classification_report_imbalanced\n",
        "from sklearn.metrics import precision_score, recall_score, f1_score, roc_auc_score, accuracy_score, classification_report\n",
        "from collections import Counter\n",
        "from sklearn.model_selection import KFold, StratifiedKFold\n",
        "import warnings\n",
        "warnings.filterwarnings(\"ignore\")\n",
        "\n",
        "\n",
        "df = pd.read_csv('/creditcard.csv')\n",
        "df.head()"
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "error",
          "ename": "FileNotFoundError",
          "evalue": "[Errno 2] No such file or directory: '/creditcard.csv'",
          "traceback": [
            "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
            "\u001b[0;31mFileNotFoundError\u001b[0m                         Traceback (most recent call last)",
            "\u001b[0;32m<ipython-input-2-aaf12f761845>\u001b[0m in \u001b[0;36m<cell line: 37>\u001b[0;34m()\u001b[0m\n\u001b[1;32m     35\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     36\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 37\u001b[0;31m \u001b[0mdf\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mpd\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mread_csv\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m'/creditcard.csv'\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m     38\u001b[0m \u001b[0mdf\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mhead\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m/usr/local/lib/python3.10/dist-packages/pandas/io/parsers/readers.py\u001b[0m in \u001b[0;36mread_csv\u001b[0;34m(filepath_or_buffer, sep, delimiter, header, names, index_col, usecols, dtype, engine, converters, true_values, false_values, skipinitialspace, skiprows, skipfooter, nrows, na_values, keep_default_na, na_filter, verbose, skip_blank_lines, parse_dates, infer_datetime_format, keep_date_col, date_parser, date_format, dayfirst, cache_dates, iterator, chunksize, compression, thousands, decimal, lineterminator, quotechar, quoting, doublequote, escapechar, comment, encoding, encoding_errors, dialect, on_bad_lines, delim_whitespace, low_memory, memory_map, float_precision, storage_options, dtype_backend)\u001b[0m\n\u001b[1;32m    910\u001b[0m     \u001b[0mkwds\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mupdate\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mkwds_defaults\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    911\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 912\u001b[0;31m     \u001b[0;32mreturn\u001b[0m \u001b[0m_read\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mfilepath_or_buffer\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mkwds\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    913\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    914\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m/usr/local/lib/python3.10/dist-packages/pandas/io/parsers/readers.py\u001b[0m in \u001b[0;36m_read\u001b[0;34m(filepath_or_buffer, kwds)\u001b[0m\n\u001b[1;32m    575\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    576\u001b[0m     \u001b[0;31m# Create the parser.\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 577\u001b[0;31m     \u001b[0mparser\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mTextFileReader\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mfilepath_or_buffer\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m**\u001b[0m\u001b[0mkwds\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    578\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    579\u001b[0m     \u001b[0;32mif\u001b[0m \u001b[0mchunksize\u001b[0m \u001b[0;32mor\u001b[0m \u001b[0miterator\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m/usr/local/lib/python3.10/dist-packages/pandas/io/parsers/readers.py\u001b[0m in \u001b[0;36m__init__\u001b[0;34m(self, f, engine, **kwds)\u001b[0m\n\u001b[1;32m   1405\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1406\u001b[0m         \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mhandles\u001b[0m\u001b[0;34m:\u001b[0m \u001b[0mIOHandles\u001b[0m \u001b[0;34m|\u001b[0m \u001b[0;32mNone\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0;32mNone\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m-> 1407\u001b[0;31m         \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_engine\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_make_engine\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mf\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mengine\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m   1408\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1409\u001b[0m     \u001b[0;32mdef\u001b[0m \u001b[0mclose\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m)\u001b[0m \u001b[0;34m->\u001b[0m \u001b[0;32mNone\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m/usr/local/lib/python3.10/dist-packages/pandas/io/parsers/readers.py\u001b[0m in \u001b[0;36m_make_engine\u001b[0;34m(self, f, engine)\u001b[0m\n\u001b[1;32m   1659\u001b[0m                 \u001b[0;32mif\u001b[0m \u001b[0;34m\"b\"\u001b[0m \u001b[0;32mnot\u001b[0m \u001b[0;32min\u001b[0m \u001b[0mmode\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1660\u001b[0m                     \u001b[0mmode\u001b[0m \u001b[0;34m+=\u001b[0m \u001b[0;34m\"b\"\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m-> 1661\u001b[0;31m             self.handles = get_handle(\n\u001b[0m\u001b[1;32m   1662\u001b[0m                 \u001b[0mf\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1663\u001b[0m                 \u001b[0mmode\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m/usr/local/lib/python3.10/dist-packages/pandas/io/common.py\u001b[0m in \u001b[0;36mget_handle\u001b[0;34m(path_or_buf, mode, encoding, compression, memory_map, is_text, errors, storage_options)\u001b[0m\n\u001b[1;32m    857\u001b[0m         \u001b[0;32mif\u001b[0m \u001b[0mioargs\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mencoding\u001b[0m \u001b[0;32mand\u001b[0m \u001b[0;34m\"b\"\u001b[0m \u001b[0;32mnot\u001b[0m \u001b[0;32min\u001b[0m \u001b[0mioargs\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mmode\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    858\u001b[0m             \u001b[0;31m# Encoding\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 859\u001b[0;31m             handle = open(\n\u001b[0m\u001b[1;32m    860\u001b[0m                 \u001b[0mhandle\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    861\u001b[0m                 \u001b[0mioargs\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mmode\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;31mFileNotFoundError\u001b[0m: [Errno 2] No such file or directory: '/creditcard.csv'"
          ]
        }
      ]
    },
    {
      "metadata": {
        "_kg_hide-input": true,
        "_cell_guid": "376ce881-463a-4a09-9ac0-c63f85577eec",
        "_uuid": "93031e732e5aca3a2b4984799d6bf58d76e4b52d",
        "trusted": true,
        "id": "l6JjWrspPSc2"
      },
      "cell_type": "code",
      "source": [
        "df.describe()"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "metadata": {
        "_kg_hide-input": true,
        "_cell_guid": "03ddb929-5bc8-4af4-90cd-21dcbb57560d",
        "_uuid": "38bec67888aa534e9739e95ef9fac62d27a87021",
        "trusted": true,
        "id": "kWkCmO0yPSc2"
      },
      "cell_type": "code",
      "source": [
        "# We must check if we have any null values\n",
        "df.isnull().sum().max()\n",
        "#df.dropna(axis=1, inplace=True)\n"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "metadata": {
        "_kg_hide-input": true,
        "_cell_guid": "6a526b6c-8463-4f6f-92b0-e8a3a21cbb2e",
        "_uuid": "479a5f12d3dd68262316a17b4b7b3499e0a2cbe0",
        "trusted": true,
        "id": "GQEg7Z9NPSc2"
      },
      "cell_type": "code",
      "source": [
        "df.columns"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "metadata": {
        "_kg_hide-input": true,
        "_cell_guid": "01c007fa-0fcc-4eea-84ff-0861a2f8c533",
        "_uuid": "f6b96ff34855e3bf7af1f6979342b01c473e4e07",
        "trusted": true,
        "id": "6urDT-IlPSc3"
      },
      "cell_type": "code",
      "source": [
        "# The classes are heavily skewed we need to take note of that.\n",
        "print('No Frauds', round(df['Class'].value_counts()[0]/len(df) * 100,2), '% of the dataset')\n",
        "print('Frauds', round(df['Class'].value_counts()[1]/len(df) * 100,2), '% of the dataset')"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "metadata": {
        "_cell_guid": "558c9b60-3f52-4da5-92fa-9fc4acbdbb3a",
        "_uuid": "c2bb0945a312508e908386fc87adc227f0afe0e0",
        "id": "JQVqV5M6PSc3"
      },
      "cell_type": "markdown",
      "source": [
        "**Note:**  Notice how imbalanced is our original dataset! Most of the transactions are non-fraud. If we use this dataframe as the base for our predictive models and analysis we might get a lot of errors and our algorithms will probably overfit since it will \"assume\" that most transactions are not fraud. But we don't want our model to assume, we want our model to detect patterns that give signs of fraud!"
      ]
    },
    {
      "metadata": {
        "_kg_hide-input": true,
        "_cell_guid": "657bc987-4b15-4cfa-b290-c39a2632e2ac",
        "_uuid": "337caaf6ed3f65beedb24a74eebb22d97ff52ba4",
        "trusted": true,
        "id": "WzKODDusPSc4"
      },
      "cell_type": "code",
      "source": [
        "df_card=df\n",
        "plt.figure(figsize=(13,7))\n",
        "plt.subplot(121)\n",
        "plt.title('Fraudulent BarPlot', fontweight='bold',fontsize=14)\n",
        "ax = df_card['Class'].value_counts().plot(kind='bar')\n",
        "total = float(len(df_card))\n",
        "for p in ax.patches:\n",
        "    height = p.get_height()\n",
        "    ax.text(p.get_x()+p.get_width()/2.,\n",
        "            height + 3,\n",
        "            '{:1.5f}'.format(height/total),\n",
        "            ha=\"center\")\n",
        "\n",
        "\n",
        "plt.subplot(122)\n",
        "df_card[\"Class\"].value_counts().plot.pie(autopct = \"%1.5f%%\")\n",
        "plt.show()"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "metadata": {
        "_cell_guid": "3c9973d0-83bd-4b09-860e-c1f507f88310",
        "_uuid": "6894af2afdbfd5cd670d00b66f10ae49f1cab421",
        "id": "FomRZ61gPSc4"
      },
      "cell_type": "markdown",
      "source": [
        "**Distributions:** By seeing the distributions we can have an idea how skewed are these features, we can also see further distributions of the other features. There are techniques that can help the distributions be less skewed which will be implemented in this notebook in the future."
      ]
    },
    {
      "metadata": {
        "_kg_hide-input": true,
        "_cell_guid": "cee315f2-325f-42b6-a640-736f10c272cc",
        "_uuid": "cfa51792bf6f8a6b318ae1bffcff4e922b1d1917",
        "trusted": true,
        "id": "Lafo93IoPSc4"
      },
      "cell_type": "code",
      "source": [
        "fig, ax = plt.subplots(1, 2, figsize=(18,4))\n",
        "\n",
        "amount_val = df['Amount'].values\n",
        "time_val = df['Time'].values\n",
        "\n",
        "sns.distplot(amount_val, ax=ax[0], color='r')\n",
        "ax[0].set_title('Distribution of Transaction Amount', fontsize=14)\n",
        "ax[0].set_xlim([min(amount_val), max(amount_val)])\n",
        "\n",
        "sns.distplot(time_val, ax=ax[1], color='b')\n",
        "ax[1].set_title('Distribution of Transaction Time', fontsize=14)\n",
        "ax[1].set_xlim([min(time_val), max(time_val)])\n",
        "\n",
        "\n",
        "\n",
        "plt.show()\n",
        "# Box Plot of amount for both classes\n",
        "plt.figure(figsize = (7, 6))\n",
        "a=sns.boxplot(x = 'Class', y = 'Amount',hue='Class', data = df,showfliers=False)\n",
        "plt.setp(a.get_xticklabels(), rotation=45)"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# Storing Fraud and non-Fraud transactions\n",
        "\n",
        "df_nonfraud = df_card[df_card.Class == 0]\n",
        "df_fraud = df_card[df_card.Class == 1]"
      ],
      "metadata": {
        "id": "kqcxwrRzsVNx"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "metadata": {
        "_cell_guid": "72fdda5e-7f82-488d-a433-6157d6180bb8",
        "_uuid": "c5d6781e61c0ee84e72d26e8465bfd98ef91f3b9",
        "id": "7w-uX-TTPSc5"
      },
      "cell_type": "markdown",
      "source": [
        "<h2> Scaling and Distributing </h2>\n",
        "<a id=\"distributing\"></a>\n",
        "In this phase of our kernel, we will first scale the columns comprise of <b>Time</b> and <b>Amount </b>. Time and amount should be scaled as the other columns. On the other hand, we need to also create a sub sample of the dataframe in order to have an equal amount of Fraud and Non-Fraud cases, helping our algorithms better understand patterns that determines whether a transaction is a fraud or not.\n",
        "\n",
        "<h3> What is a sub-Sample?</h3>\n",
        "In this scenario, our subsample will be a dataframe with a 50/50 ratio of fraud and non-fraud transactions. Meaning our sub-sample will have the same amount of fraud and non fraud transactions.\n",
        "\n",
        "<h3> Why do we create a sub-Sample?</h3>\n",
        "In the beginning of this notebook we saw that the original dataframe was heavily imbalanced! Using the original dataframe  will cause the following issues:\n",
        "<ul>\n",
        "<li><b>Overfitting: </b>Our classification models will assume that in most cases there are no frauds! What we want for our model is to be certain when a fraud occurs. </li>\n",
        "<li><b>Wrong Correlations:</b> Although we don't know what the \"V\" features stand for, it will be useful to understand how each of this features influence the result (Fraud or No Fraud) by having an imbalance dataframe we are not able to see the true correlations between the class and features. </li>\n",
        "</ul>\n",
        "\n",
        "<h3>Summary: </h3>\n",
        "<ul>\n",
        "<li> <b>Scaled amount </b> and <b> scaled time </b> are the columns with scaled values. </li>\n",
        "<li> There are <b>492 cases </b> of fraud in our dataset so we can randomly get 492 cases of non-fraud to create our new sub dataframe. </li>\n",
        "<li>We concat the 492 cases of fraud and non fraud, <b>creating a new sub-sample. </b></li>\n",
        "</ul>"
      ]
    },
    {
      "metadata": {
        "_kg_hide-input": true,
        "_cell_guid": "d5d64bf0-2fbb-4096-a265-f68887bf2fde",
        "_uuid": "1501ec379c9b5c39c3857ba0febd0aedee9c30d5",
        "trusted": true,
        "id": "PXUq9cOKPSc6"
      },
      "cell_type": "code",
      "source": [
        "# Since most of our data has already been scaled we should scale the columns that are left to scale (Amount and Time)\n",
        "from sklearn.preprocessing import StandardScaler, RobustScaler\n",
        "# RobustScaler is less prone to outliers.\n",
        "std_scaler = StandardScaler()\n",
        "rob_scaler = RobustScaler()\n",
        "df['scaled_amount'] = rob_scaler.fit_transform(df['Amount'].values.reshape(-1,1))\n",
        "df['scaled_time'] = rob_scaler.fit_transform(df['Time'].values.reshape(-1,1))\n",
        "df.drop(['Time','Amount'], axis=1, inplace=True)"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "Difference between robust and standard scaler :\n",
        "\n",
        "1.   Standard_scaler:removes the mean\n",
        "and scales the data to unit variance. The scaling shrinks the range of the feature values. However, the outliers have an influence when computing the empirical mean and standard deviation. Note in particular that because the outliers on each feature have different magnitudes, the spread of the transformed data on each feature is very different.\n",
        "2.   Robust_scaler: it is based on percentiles and are therefore not influenced by a small number of very large marginal outliers. Consequently, the resulting range of the transformed feature values is larger than for the previous scalers and, more importantly, are approximately similar:\n",
        "Since there's a presence of outilers in both the time and amount features, we choose the robust scaler since standard_scaler guarantee balanced feature scales in the presence of outliers.\n",
        "\n",
        "\n",
        "  "
      ],
      "metadata": {
        "id": "or8R_xjhaEk6"
      }
    },
    {
      "metadata": {
        "_kg_hide-input": true,
        "_cell_guid": "cdb9bb1e-9fab-4fd1-a409-468ba8bc36ee",
        "_uuid": "a33d701247ab45d849c5e94735346a738a6c6970",
        "trusted": true,
        "id": "x8lpnNXfPSc6"
      },
      "cell_type": "code",
      "source": [
        "scaled_amount = df['scaled_amount']\n",
        "scaled_time = df['scaled_time']\n",
        "\n",
        "df.drop(['scaled_amount', 'scaled_time'], axis=1, inplace=True)\n",
        "df.insert(0, 'scaled_amount', scaled_amount)\n",
        "df.insert(1, 'scaled_time', scaled_time)\n",
        "\n",
        "# Amount and Time are Scaled!\n",
        "\n",
        "df.describe()"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "metadata": {
        "id": "Y1ZLxrnNJs6_"
      },
      "cell_type": "markdown",
      "source": [
        "![](https://www.xenonstack.com/wp-content/uploads/xenonstack-credit-card-fraud-detection.png)"
      ]
    },
    {
      "metadata": {
        "_cell_guid": "956d34b9-e562-4b70-a2f8-fbe060273a83",
        "_uuid": "cc554c4ffec656cb38d01c034f2cd338e1cb4565",
        "id": "WJdef8yYPSc7"
      },
      "cell_type": "markdown",
      "source": [
        "## Random Under-Sampling:\n",
        "<img src=\"http://contrib.scikit-learn.org/imbalanced-learn/stable/_images/sphx_glr_plot_random_under_sampler_001.png\">\n",
        "\n",
        "In this phase of the project we will implement *\"Random Under Sampling\"* which basically consists of removing data in order to have a more <b> balanced dataset </b> and thus avoiding our models to overfitting.\n",
        "\n",
        "#### Steps:\n",
        "<ul>\n",
        "<li>The first thing we have to do is determine how <b>imbalanced</b> is our class (use \"value_counts()\" on the class column to determine the amount for each label)  </li>\n",
        "<li>Once we determine how many instances are considered <b>fraud transactions </b> (Fraud = \"1\") , we should bring the <b>non-fraud transactions</b> to the same amount as fraud transactions (assuming we want a 50/50 ratio), this will be equivalent to 492 cases of fraud and 492 cases of non-fraud transactions.  </li>\n",
        "<li> After implementing this technique, we have a sub-sample of our dataframe with a 50/50 ratio with regards to our classes. Then the next step we will implement is to <b>shuffle the data</b> to see if our models can maintain a certain accuracy everytime we run this script.</li>\n",
        "</ul>\n",
        "\n",
        "**Note:** The main issue with \"Random Under-Sampling\" is that we run the risk that our classification models will not perform as accurate as we would like to since there is a great deal of <b>information loss</b> (bringing 492 non-fraud transaction  from 284,315 non-fraud transaction)"
      ]
    },
    {
      "metadata": {
        "_kg_hide-input": true,
        "_cell_guid": "f0acfc44-eb2a-4356-ad03-d0c12807acd7",
        "_uuid": "e3a2b89752681164f14c8273452fc66734d7f41b",
        "trusted": true,
        "id": "iZgl5-zCPSc8"
      },
      "cell_type": "code",
      "source": [
        "# Since our classes are highly skewed we should make them equivalent in order to have a normal distribution of the classes.\n",
        "\n",
        "# Lets shuffle the data before creating the subsamples\n",
        "\n",
        "df = df.sample(frac=1)\n",
        "\n",
        "# amount of fraud classes 492 rows.\n",
        "fraud_df = df.loc[df['Class'] == 1]\n",
        "non_fraud_df = df.loc[df['Class'] == 0][:492]\n",
        "\n",
        "normal_distributed_df = pd.concat([fraud_df, non_fraud_df])\n",
        "\n",
        "# Shuffle dataframe rows\n",
        "new_df = normal_distributed_df.sample(frac=1, random_state=42)\n",
        "\n",
        "new_df.head()"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "metadata": {
        "_cell_guid": "77198464-c0f8-4694-ac0b-4b29b94d0da3",
        "_uuid": "b6818122806657e7accb8be1f4bf17086bb9b149",
        "id": "OaGzse14PSc8"
      },
      "cell_type": "markdown",
      "source": [
        "##  Equally Distributing and Correlating:\n",
        "<a id=\"correlating\"></a>\n",
        "Now that we have our dataframe correctly balanced, we can go further with our <b>analysis</b> and <b>data preprocessing</b>."
      ]
    },
    {
      "metadata": {
        "_kg_hide-input": true,
        "_cell_guid": "73454100-dc69-49fd-b1b2-f72e326bca5d",
        "_uuid": "68b42e92df59f10fbd3ba700389796c4506af604",
        "trusted": true,
        "id": "3r2LVdcpPSc8"
      },
      "cell_type": "code",
      "source": [
        "import seaborn as sns\n",
        "import matplotlib.pyplot as plt\n",
        "\n",
        "print('Distribution of the Classes in the subsample dataset')\n",
        "print(new_df['Class'].value_counts() / len(new_df))\n",
        "\n",
        "# Define custom colors\n",
        "colors = [\"#1f77b4\", \"#ff7f0e\"]\n",
        "\n",
        "sns.countplot(x='Class', data=new_df, palette=colors)\n",
        "plt.title('Equally Distributed Classes', fontsize=14)\n",
        "plt.show()\n",
        "\n"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "metadata": {
        "_cell_guid": "0abc31ee-a78e-43af-822f-f06772d00c1c",
        "_uuid": "88477bac6687f110e9d64ec22837c250d85d2a2b",
        "id": "fZfBaqDKPSc8"
      },
      "cell_type": "markdown",
      "source": [
        "<h3> Correlation Matrices </h3>\n",
        "Correlation matrices are the essence of understanding our data. We want to know if there are features that influence heavily in whether a specific transaction is a fraud. However, it is important that we use the correct dataframe (subsample)  in order for us to see which features have a high positive or negative correlation with regards to fraud transactions.\n",
        "\n",
        "### Summary and Explanation:\n",
        "<ul>\n",
        "<li><b>Negative Correlations: </b>V17, V14, V12 and V10 are negatively correlated. Notice how the lower these values are, the more likely the end result will be a fraud transaction.  </li>\n",
        "<li> <b> Positive Correlations: </b> V2, V4, V11, and V19 are positively correlated. Notice how the higher these values are, the more likely the end result will be a fraud transaction. </li>\n",
        "<li> <b>BoxPlots: </b>  We will use boxplots to have a better understanding of the distribution of these features in fradulent and non fradulent transactions. </li>\n",
        "</ul>\n",
        "\n",
        "\n",
        "**Note: ** We have to make sure we use the subsample in our correlation matrix or else our correlation matrix will be affected by the high imbalance between our classes. This occurs due to the high class imbalance in the original dataframe."
      ]
    },
    {
      "metadata": {
        "_kg_hide-input": true,
        "_cell_guid": "9f353623-9435-4bb2-b854-b4a201ec7dd9",
        "_uuid": "e2f417c5d7c633a1e3cdfaa78acd6bd77a38400e",
        "trusted": true,
        "id": "hmeVcCsMPSc9"
      },
      "cell_type": "code",
      "source": [
        "# Make sure we use the subsample in our correlation\n",
        "\n",
        "f, (ax1, ax2) = plt.subplots(2, 1, figsize=(24,20))\n",
        "\n",
        "# Entire DataFrame\n",
        "corr = df.corr()\n",
        "sns.heatmap(corr, cmap='coolwarm_r', annot_kws={'size':20}, ax=ax1)\n",
        "ax1.set_title(\"Imbalanced Correlation Matrix \\n (don't use for reference)\", fontsize=14)\n",
        "\n",
        "\n",
        "sub_sample_corr = new_df.corr()\n",
        "sns.heatmap(sub_sample_corr, cmap='coolwarm_r', annot_kws={'size':20}, ax=ax2)\n",
        "ax2.set_title('SubSample Correlation Matrix \\n (use for reference)', fontsize=14)\n",
        "plt.show()"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "metadata": {
        "_kg_hide-input": true,
        "_cell_guid": "2f02c21f-daa3-4251-a8e9-acad09a5ce0f",
        "_uuid": "318d0e7e0443f99139be21c00a7abc663be26385",
        "trusted": true,
        "id": "R0nBPZMKPSc9"
      },
      "cell_type": "code",
      "source": [
        "f, axes = plt.subplots(ncols=4, figsize=(20,4))\n",
        "\n",
        "# Negative Correlations with our Class (The lower our feature value the more likely it will be a fraud transaction)\n",
        "sns.boxplot(x=\"Class\", y=\"V17\", data=new_df, palette=colors, ax=axes[0])\n",
        "axes[0].set_title('V17 vs Class Negative Correlation')\n",
        "\n",
        "sns.boxplot(x=\"Class\", y=\"V14\", data=new_df, palette=colors, ax=axes[1])\n",
        "axes[1].set_title('V14 vs Class Negative Correlation')\n",
        "\n",
        "\n",
        "sns.boxplot(x=\"Class\", y=\"V12\", data=new_df, palette=colors, ax=axes[2])\n",
        "axes[2].set_title('V12 vs Class Negative Correlation')\n",
        "\n",
        "\n",
        "sns.boxplot(x=\"Class\", y=\"V10\", data=new_df, palette=colors, ax=axes[3])\n",
        "axes[3].set_title('V10 vs Class Negative Correlation')\n",
        "\n",
        "plt.show()"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "metadata": {
        "_kg_hide-input": true,
        "_cell_guid": "b457b10e-c17c-4cb2-9719-6d4128377c9f",
        "_uuid": "7bfc46c028f8602ee949de83629082633aa47b2c",
        "trusted": true,
        "id": "zccUYkFpPSc9"
      },
      "cell_type": "code",
      "source": [
        "f, axes = plt.subplots(ncols=4, figsize=(20,4))\n",
        "\n",
        "# Positive correlations (The higher the feature the probability increases that it will be a fraud transaction)\n",
        "sns.boxplot(x=\"Class\", y=\"V11\", data=new_df, palette=colors, ax=axes[0])\n",
        "axes[0].set_title('V11 vs Class Positive Correlation')\n",
        "\n",
        "sns.boxplot(x=\"Class\", y=\"V4\", data=new_df, palette=colors, ax=axes[1])\n",
        "axes[1].set_title('V4 vs Class Positive Correlation')\n",
        "\n",
        "\n",
        "sns.boxplot(x=\"Class\", y=\"V2\", data=new_df, palette=colors, ax=axes[2])\n",
        "axes[2].set_title('V2 vs Class Positive Correlation')\n",
        "\n",
        "\n",
        "sns.boxplot(x=\"Class\", y=\"V19\", data=new_df, palette=colors, ax=axes[3])\n",
        "axes[3].set_title('V19 vs Class Positive Correlation')\n",
        "\n",
        "plt.show()"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "metadata": {
        "id": "_4DLzoQLJs7j"
      },
      "cell_type": "markdown",
      "source": [
        "<a id='4'></a><br>\n",
        "##  Splitting the Data into Training and Testing Sets\n",
        "\n",
        "\n",
        "As we know, the first basic step for regression is performing a train-test split."
      ]
    },
    {
      "metadata": {
        "trusted": true,
        "id": "_njh6c8ZJs7m"
      },
      "cell_type": "code",
      "source": [
        "# Putting the feature variable into X\n",
        "df_card=df\n",
        "X = df_card.drop(['Class'],axis = 1)\n",
        "X.head(2)"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "metadata": {
        "trusted": true,
        "id": "MkkjMuyWJs7p"
      },
      "cell_type": "code",
      "source": [
        "# Putting the Target variable to y\n",
        "\n",
        "y = df_card['Class']\n",
        "df_card.describe()"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "metadata": {
        "trusted": true,
        "id": "8oTs4_ybJs7r"
      },
      "cell_type": "code",
      "source": [
        "from sklearn.model_selection import train_test_split\n",
        "\n",
        "# Assuming X and y are defined and are your feature and target variables respectively.\n",
        "\n",
        "# Split the dataset into training and testing sets using train_test_split\n",
        "X_train, X_test, y_train, y_test = train_test_split(X, y, test_size=0.2, random_state=42)\n",
        "\n",
        "# Optional: Print shapes to verify\n",
        "print(\"X_train shape:\", X_train.shape)\n",
        "print(\"X_test shape:\", X_test.shape)\n",
        "print(\"y_train shape:\", y_train.shape)\n",
        "print(\"y_test shape:\", y_test.shape)\n"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "metadata": {
        "id": "RQc0I3ebJs76"
      },
      "cell_type": "markdown",
      "source": [
        "<a id='5'></a><br>\n",
        "##  Building a Logistic Model"
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "Here,instead of `Accuracy` we are very much interested in the recall score, because that is the metric that will help us try to capture the most fraudulent transactions.\n",
        "Increase of Recall comes at a price of Precision. In this case predicting a transaction fradulant which actually is not is not a big concern."
      ],
      "metadata": {
        "id": "JpsQjjStzL9D"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "# Evaluation Metrics\n",
        "from sklearn.metrics import classification_report\n",
        "from sklearn.metrics import confusion_matrix\n",
        "print('Classification report:\\n', classification_report(Y_test, Y_test_prediction))\n",
        "print('Confusion matrix:\\n',confusion_matrix(y_true = Y_test, y_pred = Y_test_prediction))\n",
        "print(\"Logistic Regression Accuracy: \",accuracy_score(Y_test,Y_test_prediction))\n",
        "print('ROC AUC : ', roc_auc_score(Y_test, Y_test_prediction))"
      ],
      "metadata": {
        "id": "Z2MzwoGeBUse"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "Logistic regression on balanced data"
      ],
      "metadata": {
        "id": "hPkfxsXXcm_M"
      }
    },
    {
      "metadata": {
        "trusted": true,
        "id": "wm7qouEaJs7x"
      },
      "cell_type": "code",
      "source": [
        "from sklearn import preprocessing\n",
        "from sklearn.preprocessing import PowerTransformer"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "metadata": {
        "trusted": true,
        "id": "hlCSFXK4Js71"
      },
      "cell_type": "code",
      "source": [
        "pt = preprocessing.PowerTransformer(copy=False)\n",
        "PWTR_X = pt.fit_transform(X)"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "model = LogisticRegression()"
      ],
      "metadata": {
        "id": "YOPmtDvWji8y"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# Count the occurrences of each class (0 for normal, 1 for fraud)\n",
        "classes = df_card['Class'].value_counts()\n",
        "\n",
        "# Separate the data for analysis\n",
        "non_fraud_df = df_card[df_card['Class'] == 0]  # DataFrame containing only non-fraud transactions\n",
        "fraud_df = df_card[df_card['Class'] == 1]     # DataFrame containing only fraud transactions\n",
        "\n",
        "# Print the shape of each DataFrame\n",
        "print(non_fraud_df.shape)\n",
        "print(fraud_df.shape)\n"
      ],
      "metadata": {
        "id": "LrjtiR1cjrxR"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# Undersampling ( we already did it before)\n",
        "non_fraud_sample = non_fraud_df.sample(n=492)\n",
        "new_dataset = pd.concat([non_fraud_sample, fraud_df], axis=0)\n",
        "print(new_dataset['Class'].value_counts())\n"
      ],
      "metadata": {
        "id": "QeQwLDxujrk4"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "X = new_dataset.drop(columns='Class', axis=1)  # Features (independent variables)\n",
        "Y = new_dataset['Class']                      # Target variable (dependent variable)\n",
        "\n",
        "# Separation into training and testing sets\n",
        "X_train, X_test, Y_train, Y_test = train_test_split(X, Y, test_size=0.2, random_state=42)\n",
        "\n",
        "Y_train.head()  # Print the first few rows of the training set's target variable to verify\n"
      ],
      "metadata": {
        "id": "I7V1dG_6jrYV"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# distribution of legit transactions & fraudulent transactions\n",
        "Y_train.value_counts()"
      ],
      "metadata": {
        "id": "jm5ecEYXmMJR"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# training the Logistic Regression Model with Training Data\n",
        "model.fit(X_train, Y_train)"
      ],
      "metadata": {
        "id": "5QDwh8V7mO69"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "#Evaluating the model:\n",
        "Y_train_prediction = model.predict(X_train)\n",
        "training_data_accuracy = accuracy_score(Y_train_prediction, Y_train)"
      ],
      "metadata": {
        "id": "qwEX-rpDmRtK"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "print('Accuracy on Training data : ', training_data_accuracy)"
      ],
      "metadata": {
        "id": "mVwbiHfumYQh"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "\n",
        "# accuracy on test data\n",
        "Y_test_prediction = model.predict(X_test)\n",
        "test_data_accuracy = accuracy_score(Y_test_prediction, Y_test)\n"
      ],
      "metadata": {
        "id": "QbWaPJ0zmYMe"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "\n",
        "print('Accuracy score on Test Data : ', test_data_accuracy)"
      ],
      "metadata": {
        "id": "5ZuWnwS7mYG5"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "metadata": {
        "id": "rqK8iFHzJs8S"
      },
      "cell_type": "markdown",
      "source": [
        "## Imbalanced Data Set"
      ]
    },
    {
      "metadata": {
        "id": "j1uM5gP0Js8T"
      },
      "cell_type": "markdown",
      "source": [
        "####  Logistic Regression on Imbalanced Data"
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "df_card=df\n",
        "X = df_card.drop(['Class'],axis = 1)\n",
        "y = df_card['Class']"
      ],
      "metadata": {
        "id": "8NXcSDMONIOI"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "\n",
        "\n",
        "# Assuming X and y are defined and are your feature and target variables respectively.\n",
        "# Split the dataset into training and testing sets using train_test_split\n",
        "X_train, X_test, y_train, y_test = train_test_split(X, y, test_size=0.2, random_state=42)\n",
        "\n",
        "# Optional: Print shapes to verify\n",
        "print(\"X_train shape:\", X_train.shape)\n",
        "print(\"X_test shape:\", X_test.shape)\n",
        "print(\"y_train shape:\", y_train.shape)\n",
        "print(\"y_test shape:\", y_test.shape)\n"
      ],
      "metadata": {
        "id": "vKAYpRD43L1D"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "pt = preprocessing.PowerTransformer(copy=False)\n",
        "PWTR_X = pt.fit_transform(X)"
      ],
      "metadata": {
        "id": "J-u7A9Jgyaxs"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# Splitting dataset into test and train sets in 70:30 ratio after applying Power Transform\n",
        "\n",
        "# Assuming X and y are defined and are your feature and target variables respectively.\n",
        "# Split the dataset into training and testing sets using train_test_split\n",
        "X_train, X_test, y_train, y_test = train_test_split(X, y, test_size=0.2, random_state=42)\n",
        "\n",
        "# Optional: Print shapes to verify\n",
        "print(\"X_train shape:\", X_train.shape)\n",
        "print(\"X_test shape:\", X_test.shape)\n",
        "print(\"y_train shape:\", y_train.shape)\n",
        "print(\"y_test shape:\", y_test.shape)"
      ],
      "metadata": {
        "id": "tMMS2SmYypLW"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "metadata": {
        "trusted": true,
        "id": "YIgq7sp2Js9Y"
      },
      "cell_type": "code",
      "source": [
        "from sklearn.linear_model import LogisticRegression\n",
        "\n",
        "# Fit a logistic regression model to train data\n",
        "model_lr = LogisticRegression()\n",
        "model_lr=model_lr.fit(X_train, y_train)\n"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "metadata": {
        "trusted": true,
        "id": "dAD_BwRSJs9Y"
      },
      "cell_type": "code",
      "source": [
        "# Predict on test data\n",
        "y_predicted = model_lr.predict(X_test)\n"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "metadata": {
        "trusted": true,
        "id": "eNFudOReJs9Z"
      },
      "cell_type": "code",
      "source": [
        "# Evaluation Metrics\n",
        "from sklearn.metrics import classification_report\n",
        "from sklearn.metrics import confusion_matrix\n",
        "print('Classification report:\\n', classification_report(y_test, y_predicted))\n",
        "print('Confusion matrix:\\n',confusion_matrix(y_true = y_test, y_pred = y_predicted))\n",
        "print(\"Logistic Regression Accuracy: \",accuracy_score(y_test,y_predicted))\n",
        "print('ROC AUC : ', roc_auc_score(y_test, y_predicted))\n"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "print('Classification report:\\n', classification_report(y_test, y_predicted))\n",
        "cm = confusion_matrix(y_test, y_predicted)\n",
        "print('Confusion matrix:\\n', cm)\n",
        "print(\"Logistic Regression Accuracy: \", accuracy_score(y_test, y_predicted))\n",
        "print('ROC AUC : ', roc_auc_score(y_test, y_predicted))\n",
        "\n"
      ],
      "metadata": {
        "id": "Hhvobflor6UW"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "Classification Report:\n",
        "\n",
        "Precision: Precision indicates the proportion of correctly predicted positive cases among all predicted positive cases. In this case, for class 1 (fraud), the precision is 0.86, suggesting that 86% of the transactions classified as fraud are indeed fraudulent.\n",
        "Recall: Recall (also known as sensitivity) indicates the proportion of correctly predicted positive cases among all actual positive cases. A recall of 0.60 for class 1 means that 60% of actual fraud cases are correctly identified by the model.\n",
        "F1-score: F1-score is the harmonic mean of precision and recall. It provides a balance between precision and recall. The F1-score for class 1 is 0.71."
      ],
      "metadata": {
        "id": "Dp5co9frtvwW"
      }
    },
    {
      "metadata": {
        "id": "idjQEfO7Js9b"
      },
      "cell_type": "markdown",
      "source": [
        "#### Inference:\n",
        "- Precision : 0.86\n",
        "- Recall : 0.60\n",
        "- F1-score : 0.70\n",
        "- Accuracy : 0.85\n",
        "- ROC AUC : 0.80"
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "# XBGoost Method\n",
        "\n",
        "### Building the XGBoost Model"
      ],
      "metadata": {
        "id": "KF_IIn8GhpBn"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "feature_names = df.iloc[:, 1:30].columns\n",
        "target = df.iloc[:1, 30:].columns\n",
        "\n",
        "data_features = df[feature_names]\n",
        "data_target = df[target]"
      ],
      "metadata": {
        "id": "9fvhKORg0rYE"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "feature_names"
      ],
      "metadata": {
        "id": "M3O2JndH0vyN"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "target"
      ],
      "metadata": {
        "id": "Ryms7ojA05Wo"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "from sklearn.model_selection import train_test_split\n",
        "np.random.seed(123)\n",
        "X_train_xgboost, X_test_xgboost, y_train_xgboost, y_test_xgboost = train_test_split(data_features, data_target,\n",
        "                                                    train_size = 0.80, test_size = 0.20, random_state = 1)"
      ],
      "metadata": {
        "id": "8GoCHTna1AzB"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "import xgboost as xgb\n",
        "xg = xgb.XGBClassifier()"
      ],
      "metadata": {
        "id": "30COzySJoQE9"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "### Training the Model"
      ],
      "metadata": {
        "id": "nNvECwAqooU4"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "xg.fit(X_train_xgboost, y_train_xgboost)"
      ],
      "metadata": {
        "id": "QtM4L6zryuL-"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "### Confusion Matrix - Model performance measures"
      ],
      "metadata": {
        "id": "PNVbIHQWzRFa"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "def PrintStats(cmat, y_test, pred):\n",
        "    tpos = cmat[0][0]\n",
        "    fneg = cmat[1][1]\n",
        "    fpos = cmat[0][1]\n",
        "    tneg = cmat[1][0]"
      ],
      "metadata": {
        "id": "1GJFMSSZzTBp"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "def RunModel(model, X_train, y_train, X_test, y_test):\n",
        "    model.fit(X_train, y_train.values.ravel())\n",
        "    pred = model.predict(X_test)\n",
        "    matrix = confusion_matrix(y_test, pred)\n",
        "    return matrix, pred"
      ],
      "metadata": {
        "id": "cqwmHJYRzVhk"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "### Classification Report - Model performance measures"
      ],
      "metadata": {
        "id": "CBAQVP6zzmJx"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "!pip install scikit-plot\n",
        "from sklearn.metrics import accuracy_score, confusion_matrix, classification_report, roc_auc_score, roc_curve\n",
        "import scikitplot as skplt"
      ],
      "metadata": {
        "id": "gJ7WQfkMzn8x"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "cmat, pred = RunModel(xg, X_train_xgboost, y_train_xgboost, X_test_xgboost, y_test_xgboost)"
      ],
      "metadata": {
        "id": "V97M6nDIztXb"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "import scikitplot as skplt\n",
        "skplt.metrics.plot_confusion_matrix(y_test_xgboost, pred)"
      ],
      "metadata": {
        "id": "MKkhLVEx1p8n"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "accuracy_score(y_test_xgboost, pred)"
      ],
      "metadata": {
        "id": "etzryveJ1uaR"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "print (classification_report(y_test_xgboost, pred))"
      ],
      "metadata": {
        "id": "EI_m_fp6BILv"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "## Undersampling and Oversampling - Working with unbalanced data"
      ],
      "metadata": {
        "id": "jVINqltL9N6W"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "The application of methods for data balancing, such as undersampling and oversampling techniques are widely used in these cases. Changing the sampling makes the algorithm more \"sensitive\" to fraudulent transactions.\n",
        "\n",
        "Undersampling is the technique of removing major class records from the sample. In this case, it is necessary to remove random records from the legitimate class (No fraud), in order to obtain a number of records close to the amount of the minority class (fraud) in order to train the model.\n",
        "\n",
        "Oversampling is exactly the opposite: it means adding minority class records (fraud) to our training sample, thus increasing the overall proportion of fraud records. There are methods to generate samples from the minority class, either by duplicating existing records or artificially generating others.\n",
        "\n",
        "In this case, we will use the undersampling technique to obtain a uniform division between fraud and valid transactions. This will make the training set small, but with enough data to generate a good classifier."
      ],
      "metadata": {
        "id": "qyJj2GqJ9PFE"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "### Applying the undersampling technique"
      ],
      "metadata": {
        "id": "udZ7Q_yN9hnL"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "# Lets shuffle the data before creating the subsamples\n",
        "df = df.sample(frac=1)\n",
        "\n",
        "# amount of fraud classes 492 rows.\n",
        "fraud_df2 = df.loc[df['Class'] == 1]\n",
        "non_fraud_df2 = df.loc[df['Class'] == 0][:492]\n",
        "\n",
        "normal_distributed_df2 = pd.concat([fraud_df2, non_fraud_df2])\n",
        "\n",
        "# Shuffle dataframe rows\n",
        "new_df2 = normal_distributed_df2.sample(frac=1, random_state=42)\n"
      ],
      "metadata": {
        "id": "YzdGiRJN_cbT"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "feature_names = new_df2.iloc[:, 1:30].columns\n",
        "target = new_df2.iloc[:1, 30:].columns\n",
        "\n",
        "data_features = new_df2[feature_names]\n",
        "data_target = new_df2[target]"
      ],
      "metadata": {
        "id": "xLNGY_ew_1Ze"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "feature_names"
      ],
      "metadata": {
        "id": "UFM9u40iAEhQ"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "target"
      ],
      "metadata": {
        "id": "Q8kOrNgU9Lck"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "np.random.seed(123)\n",
        "X_undersampled_train, X_undersampled_test, Y_undersampled_train, Y_undersampled_test = train_test_split(data_features, data_target,\n",
        "                                                    train_size = 0.80, test_size = 0.20, random_state = 1)"
      ],
      "metadata": {
        "id": "d23g7BWaAOcj"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "### Using the \"new\" classifier for balanced data"
      ],
      "metadata": {
        "id": "i8B2csfv-Gr7"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "xg_undersampled = xgb.XGBClassifier()\n",
        "cmat, pred = RunModel(xg_undersampled, X_undersampled_train, Y_undersampled_train, X_undersampled_test, Y_undersampled_test)\n",
        "PrintStats(cmat, Y_undersampled_test, pred)\n"
      ],
      "metadata": {
        "id": "M4NlgwcI-HpD"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "skplt.metrics.plot_confusion_matrix(Y_undersampled_test, pred)"
      ],
      "metadata": {
        "id": "7Ie-N2xR-MoC"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "accuracy_score(Y_undersampled_test, pred)"
      ],
      "metadata": {
        "id": "IgVruPn2AqFG"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "print (classification_report(Y_undersampled_test, pred))"
      ],
      "metadata": {
        "id": "L5dyfdn1Aq9K"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "Accuracy has decreased, but sensitivity has greatly increased. Looking at the confusion matrix, we can see a much higher percentage of correct classifications of fraudulent data.\n",
        "\n",
        "Unfortunately, a greater number of fraud classifications almost always means a correspondingly greater number of valid transactions also classified as fraudulent."
      ],
      "metadata": {
        "id": "w6_B-mXwBW3l"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "### Using the \"new\" classifier for the original data test"
      ],
      "metadata": {
        "id": "7rNx6YirByAe"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "xg = xgb.XGBClassifier()\n",
        "cmat, pred = RunModel(xg, X_undersampled_train, Y_undersampled_train, X_test_xgboost, y_test_xgboost)\n",
        "PrintStats(cmat, y_test_xgboost, pred)"
      ],
      "metadata": {
        "id": "ClDryCn_B1Bf"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "skplt.metrics.plot_confusion_matrix(y_test_xgboost, pred)"
      ],
      "metadata": {
        "id": "knBCxo3pCEpC"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "accuracy_score(y_test_xgboost, pred)"
      ],
      "metadata": {
        "id": "yJOFlKV7C4eY"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "print (classification_report(y_test_xgboost, pred))"
      ],
      "metadata": {
        "id": "iC9V5ryQDBwa"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "### Measurement of classifier performance through the ROC and AUC curve"
      ],
      "metadata": {
        "id": "QWVgySraDIOS"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "The \"ROC\" curve is a probability curve that shows how much the classifier can distinguish between two things, through two parameters: the true-positive rate versus the false-positive rate, that is, the number of times the classifier hit the prediction against the number of times the classifier missed the prediction.\n",
        "\n",
        "The \"AUC\" is derived from the \"ROC\" curve and represents the degree or measure of separability. The AUC summarizes the ROC curve in a single value, calculating the â€œarea under the curveâ€. The higher the AUC the better the model is in predicting 0s as 0s and 1s as 1s. In this case, the higher the AUC the better the model is in distinguishing between fraudulent and normal transactions. The AUC value ranges from 0.0 to 1.0.\n",
        "\n",
        "An excellent model has AUC close to 1, which means it has a good measure of separability. A poor model has AUC close to 0, which means that it has the worst measure of separability, that is, it is predicting 0s as 1s and 1s as 0s. And when the AUC is 0.5, it means that the model has no class separation capability."
      ],
      "metadata": {
        "id": "WlbecCR_DLdD"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "from sklearn import metrics"
      ],
      "metadata": {
        "id": "5oVzWiPoDHkU"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# Creating XGBoost model\n",
        "clf = xgb.XGBClassifier()\n",
        "clf.fit(X_undersampled_train, Y_undersampled_train)\n",
        "y_pred = clf.predict(X_test_xgboost)\n",
        "\n",
        "# AUC Curve XGBoost\n",
        "y_pred_probability = clf.predict_proba(X_test_xgboost)[::,1]\n",
        "fpr, tpr, _ = metrics.roc_curve(y_test_xgboost, y_pred_probability)\n",
        "auc = metrics.roc_auc_score(y_test_xgboost, y_pred_probability)\n",
        "plt.plot(fpr,tpr,label=\"XGBoost, auc=\"+str(auc))\n",
        "plt.legend(loc=4)\n",
        "plt.show()"
      ],
      "metadata": {
        "id": "RjSUPVDbDRgU"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "The classifier had a very good result, with AUC of 0.99!"
      ],
      "metadata": {
        "id": "51imtypSDkds"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "#Artificial Neural Network (ANNs)"
      ],
      "metadata": {
        "id": "09z2CzqiIgKa"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "from sklearn.model_selection import train_test_split\n",
        "from sklearn.preprocessing import StandardScaler\n",
        "\n",
        "scalar = RobustScaler()\n",
        "\n",
        "X = df.drop('Class', axis=1)\n",
        "y = df.Class\n",
        "\n",
        "X_train_v, X_test, y_train_v, y_test = train_test_split(X, y,\n",
        "                                                    test_size=0.3, random_state=42)\n",
        "X_train, X_validate, y_train, y_validate = train_test_split(X_train_v, y_train_v,\n",
        "                                                            test_size=0.2, random_state=42)\n",
        "\n",
        "X_train = scalar.fit_transform(X_train)\n",
        "X_validate = scalar.transform(X_validate)\n",
        "X_test = scalar.transform(X_test)\n",
        "\n",
        "w_p = y_train.value_counts()[0] / len(y_train)\n",
        "w_n = y_train.value_counts()[1] / len(y_train)\n",
        "\n",
        "print(f\"Fraudulant transaction weight: {w_n}\")\n",
        "print(f\"Non-Fraudulant transaction weight: {w_p}\")"
      ],
      "metadata": {
        "id": "-rAcRJxFI1f7"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "print(f\"TRAINING: X_train: {X_train.shape}, y_train: {y_train.shape}\\n{'_'*55}\")\n",
        "print(f\"VALIDATION: X_validate: {X_validate.shape}, y_validate: {y_validate.shape}\\n{'_'*50}\")\n",
        "print(f\"TESTING: X_test: {X_test.shape}, y_test: {y_test.shape}\")"
      ],
      "metadata": {
        "id": "R-J7GK0vJsJH"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "from sklearn.metrics import accuracy_score, confusion_matrix, classification_report, f1_score\n",
        "\n",
        "def print_score(label, prediction, train=True):\n",
        "    if train:\n",
        "        clf_report = pd.DataFrame(classification_report(label, prediction, output_dict=True))\n",
        "        print(\"Train Result:\\n================================================\")\n",
        "        print(f\"Accuracy Score: {accuracy_score(label, prediction) * 100:.2f}%\")\n",
        "        print(\"_______________________________________________\")\n",
        "        print(f\"Classification Report:\\n{clf_report}\")\n",
        "        print(\"_______________________________________________\")\n",
        "        print(f\"Confusion Matrix: \\n {confusion_matrix(y_train, prediction)}\\n\")\n",
        "\n",
        "    elif train==False:\n",
        "        clf_report = pd.DataFrame(classification_report(label, prediction, output_dict=True))\n",
        "        print(\"Test Result:\\n================================================\")\n",
        "        print(f\"Accuracy Score: {accuracy_score(label, prediction) * 100:.2f}%\")\n",
        "        print(\"_______________________________________________\")\n",
        "        print(f\"Classification Report:\\n{clf_report}\")\n",
        "        print(\"_______________________________________________\")\n",
        "        print(f\"Confusion Matrix: \\n {confusion_matrix(label, prediction)}\\n\")"
      ],
      "metadata": {
        "id": "APYrfdOUJ2mo"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "##Model Building"
      ],
      "metadata": {
        "id": "063_epcqJ8Jt"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "X_train.shape[-1]"
      ],
      "metadata": {
        "id": "bYR41bWjJ7aX"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "from tensorflow import keras\n",
        "\n",
        "model = keras.Sequential([\n",
        "    keras.layers.Dense(256, activation='relu', input_shape=(X_train.shape[-1],)),\n",
        "    keras.layers.BatchNormalization(),\n",
        "    keras.layers.Dropout(0.3),\n",
        "    keras.layers.Dense(256, activation='relu'),\n",
        "    keras.layers.BatchNormalization(),\n",
        "    keras.layers.Dropout(0.3),\n",
        "    keras.layers.Dense(256, activation='relu'),\n",
        "    keras.layers.BatchNormalization(),\n",
        "    keras.layers.Dropout(0.3),\n",
        "    keras.layers.Dense(1, activation='sigmoid'),\n",
        "])\n",
        "\n",
        "model.summary()"
      ],
      "metadata": {
        "id": "j-eyONJjKIRB"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "METRICS = [\n",
        "#     keras.metrics.Accuracy(name='accuracy'),\n",
        "    keras.metrics.FalseNegatives(name='fn'),\n",
        "    keras.metrics.FalsePositives(name='fp'),\n",
        "    keras.metrics.TrueNegatives(name='tn'),\n",
        "    keras.metrics.TruePositives(name='tp'),\n",
        "    keras.metrics.Precision(name='precision'),\n",
        "    keras.metrics.Recall(name='recall')\n",
        "]\n",
        "\n",
        "model.compile(optimizer=keras.optimizers.Adam(1e-4), loss='binary_crossentropy', metrics=METRICS)\n",
        "\n",
        "callbacks = [keras.callbacks.ModelCheckpoint('fraud_model_at_epoch_{epoch}.h5')]\n",
        "class_weight = {0:w_p, 1:w_n}\n",
        "\n",
        "r = model.fit(\n",
        "    X_train, y_train,\n",
        "    validation_data=(X_validate, y_validate),\n",
        "    batch_size=2048,\n",
        "    epochs=300,\n",
        "#     class_weight=class_weight,\n",
        "    callbacks=callbacks,\n",
        ")"
      ],
      "metadata": {
        "id": "5-J8Ht8qKLUq"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "score = model.evaluate(X_test, y_test)\n",
        "print(score)"
      ],
      "metadata": {
        "id": "XzePV3IZKS1P"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "plt.figure(figsize=(12, 16))\n",
        "\n",
        "plt.subplot(4, 2, 1)\n",
        "plt.plot(r.history['loss'], label='Loss')\n",
        "plt.plot(r.history['val_loss'], label='val_Loss')\n",
        "plt.title('Loss Function evolution during training')\n",
        "plt.legend()\n",
        "\n",
        "plt.subplot(4, 2, 2)\n",
        "plt.plot(r.history['fn'], label='fn')\n",
        "plt.plot(r.history['val_fn'], label='val_fn')\n",
        "plt.title('Accuracy evolution during training')\n",
        "plt.legend()\n",
        "\n",
        "plt.subplot(4, 2, 3)\n",
        "plt.plot(r.history['precision'], label='precision')\n",
        "plt.plot(r.history['val_precision'], label='val_precision')\n",
        "plt.title('Precision evolution during training')\n",
        "plt.legend()\n",
        "\n",
        "plt.subplot(4, 2, 4)\n",
        "plt.plot(r.history['recall'], label='recall')\n",
        "plt.plot(r.history['val_recall'], label='val_recall')\n",
        "plt.title('Recall evolution during training')\n",
        "plt.legend()"
      ],
      "metadata": {
        "id": "Ng1EwrjlKTib"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "y_train_pred = model.predict(X_train)\n",
        "y_test_pred = model.predict(X_test)\n",
        "\n",
        "print_score(y_train, y_train_pred.round(), train=True)\n",
        "print_score(y_test, y_test_pred.round(), train=False)\n",
        "\n",
        "scores_dict = {\n",
        "    'ANNs': {\n",
        "        'Train': f1_score(y_train, y_train_pred.round()),\n",
        "        'Test': f1_score(y_test, y_test_pred.round()),\n",
        "    },\n",
        "}"
      ],
      "metadata": {
        "id": "PF-n--3AKXf_"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "CfRIqDJpRmZP",
        "outputId": "95816c14-4fcc-4850-ea0f-71840cdcd11c"
      },
      "execution_count": 2,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "\u001b[33mhint: Using 'master' as the name for the initial branch. This default branch name\u001b[m\n",
            "\u001b[33mhint: is subject to change. To configure the initial branch name to use in all\u001b[m\n",
            "\u001b[33mhint: of your new repositories, which will suppress this warning, call:\u001b[m\n",
            "\u001b[33mhint: \u001b[m\n",
            "\u001b[33mhint: \tgit config --global init.defaultBranch <name>\u001b[m\n",
            "\u001b[33mhint: \u001b[m\n",
            "\u001b[33mhint: Names commonly chosen instead of 'master' are 'main', 'trunk' and\u001b[m\n",
            "\u001b[33mhint: 'development'. The just-created branch can be renamed via this command:\u001b[m\n",
            "\u001b[33mhint: \u001b[m\n",
            "\u001b[33mhint: \tgit branch -m <name>\u001b[m\n",
            "Initialized empty Git repository in /content/.git/\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "yPzeqK1iSHPf",
        "outputId": "1f6e2dbb-73d1-463a-c964-579e9ec64739"
      },
      "execution_count": 5,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "fatal: pathspec 'Credit_Fraud_Detection.ipynb' did not match any files\n"
          ]
        }
      ]
    }
  ],
  "metadata": {
    "language_info": {
      "name": "python",
      "version": "3.6.6",
      "mimetype": "text/x-python",
      "codemirror_mode": {
        "name": "ipython",
        "version": 3
      },
      "pygments_lexer": "ipython3",
      "nbconvert_exporter": "python",
      "file_extension": ".py"
    },
    "kernelspec": {
      "display_name": "Python 3",
      "language": "python",
      "name": "python3"
    },
    "colab": {
      "provenance": [],
      "collapsed_sections": [
        "fZfBaqDKPSc8",
        "idjQEfO7Js9b"
      ]
    }
  },
  "nbformat": 4,
  "nbformat_minor": 0
}